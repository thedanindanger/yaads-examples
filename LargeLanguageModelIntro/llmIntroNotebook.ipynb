{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources\n",
    "\n",
    "- ChatGPT (what everyone is using): https://chat.openai.com/auth/login\n",
    "- OpenAI playground (what you can use when ChatGPT is down): https://platform.openai.com/playground\n",
    "- LLaMa repo: https://github.com/facebookresearch/llama\n",
    "- Alpaca repo: https://github.com/tatsu-lab/stanford_alpaca\n",
    "- BLOOM repo: https://huggingface.co/bigscience/bloom from https://github.com/bigscience-workshop/Megatron-DeepSpeed \n",
    "\n",
    "- How GPT (and LLMs generally) work: https://dugas.ch/artificial_curiosity/GPT_architecture.html \n",
    "- Colab notebook to train own LLM: https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling_from_scratch.ipynb\n",
    "- HuggingFace demo with different LLMs: https://huggingface.co/spaces/olivierdehaene/chat-llm-streaming\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChatGPT (OpenAI) Response to \"Current State of Large Language Models in 2023\"\n",
    "As of 2023, large language models have continued to evolve and advance in various ways, with several notable developments and applications:\n",
    "\n",
    "1. Continual improvement in model size and performance: Language models have continued to increase in size and complexity, with recent models such as GPT-4 and T5 exceeding trillions of parameters. These models have achieved state-of-the-art results on a range of language tasks, including language generation, translation, and understanding.\n",
    "\n",
    "2. Expansion of multilingual capabilities: Many large language models are now trained to understand and generate text in multiple languages, including low-resource languages, making them more accessible and useful for a wider range of users worldwide.\n",
    "\n",
    "3. Enhanced fine-tuning capabilities: Fine-tuning allows for the adaptation of pre-trained models to specific domains or tasks, such as conversational AI, question answering, and sentiment analysis. Recent advances in fine-tuning techniques have made it easier to fine-tune models for specific applications with fewer training examples.\n",
    "\n",
    "4. Advancements in model compression and optimization: As models grow larger, there is a need to compress and optimize them for efficient deployment in resource-constrained environments. Recent techniques such as pruning, distillation, and quantization have made it possible to reduce the size and computational cost of large models without significant loss in performance.\n",
    "\n",
    "5. Increased attention to ethical considerations: With the growing influence of large language models, there has been increasing attention to ethical considerations surrounding their use, including issues related to bias, privacy, and transparency. Researchers and practitioners are working to address these concerns through improved data collection, model training, and deployment practices.\n",
    "\n",
    "Overall, large language models have continued to push the boundaries of natural language processing and have demonstrated significant potential for a wide range of applications. However, ongoing efforts are needed to ensure their responsible and ethical use.\n",
    "\n",
    "\n",
    "\n",
    "## Analysis of response\n",
    "This response is a great example of why ChatGPT isn't going to replace people directly, it (and other Large Language Models) need fine tuning for specific use cases.\n",
    "\n",
    "This is a 'vanilla' response. It's so generic that it says effectively nothing. If someone was unfamiliar with LLM and Natural Language Processing/Understanding, they may feel this ChatGPT's response is suitable. They may even publish an article about it and present themselves as an expert.\n",
    "\n",
    "Unfortunately, such articles are then used to further train LLMs, reinforcing this 'Cycle of Stupidity'(tm). \n",
    "\n",
    "### Split in LLMs Business Models - A lot of a little or a little of a lot\n",
    "Broadly speaking, in technology there are two business models:\n",
    "1. Get a lot of users by charging a little for the service\n",
    "2. Charge a little number of users a lot of money\n",
    "\n",
    "ChatGPT and it's ilk are type 1. Type 2 are not discussed generally because it requires a lot of work, and they need to be private\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI API\n",
    "Basic config settings. Use API key from OpenAI to fine tune and run your own models\n",
    "\n",
    "Get API key from here (requires account): https://platform.openai.com/account/api-keys "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Set the API Key\n",
    "key = open(\"../.config/openai.key\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import openai\n",
    "import pandas as pd\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps\n",
    "## Try BLOOM and derivitatives: \n",
    "BLOOMZ trained BLOOM model: https://github.com/bigscience-workshop/xmtf#bloomz\n",
    "- how to download huggingface model: https://huggingface.co/docs/huggingface_hub/v0.13.3/guides/download\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a2ed677225fe28fe8c7ce28c3636dbb1e2cdff49ecd1e58fa0656d75fd999a42"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
